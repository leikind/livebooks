# Links and Monitors

## Spawning a Simple Process

Let's spawn a process which is doing nothing and is only waiting for a specific message to exit

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit -> :ok
    end
  end
end

pid = spawn(Process1, :loop, [])
```

```elixir
Process.alive?(pid)
```

```elixir
send(pid, :quit)
```

```elixir
Process.alive?(pid)
```

## Two Processes Unaware of Each Other

Without links or monitors when a process crashes it has no effect on other processes.

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit ->
        :ok

      {:spawn_process_2, sender} ->
        #                           MFA
        process2_pid = spawn(Process2, :loop, [])
        send(sender, {:process2_pid, process2_pid})
        loop()
    end
  end
end

defmodule Process2 do
  def loop do
    receive do
      :quit ->
        :ok

      :crash_and_burn ->
        _a = 1 / 0
        loop()
    end
  end
end

pid = spawn(Process1, :loop, [])
```

```elixir
Process.alive?(pid)
```

```elixir
send(pid, {:spawn_process_2, self()})

process2_pid =
  receive do
    {:process2_pid, p} -> p
  end
```

```elixir
{Process.alive?(pid), Process.alive?(process2_pid)}
```

```elixir
send(process2_pid, :crash_and_burn)
```

```elixir
{Process.alive?(pid), Process.alive?(process2_pid)}
```

```elixir
send(pid, :quit)
```

## Link Between Two Processes

When two processes are linked, when one of them crashes, the other crashes, too.

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit ->
        :ok

      {:spawn_process_2, sender} ->
        process2_pid = spawn(Process2, :loop, [])
        # ↓ ↓ ↓ the only new line compared to the previous example!
        Process.link(process2_pid)
        send(sender, {:process2_pid, process2_pid})
        loop()
    end
  end
end

defmodule Process2 do
  def loop do
    receive do
      :quit ->
        :ok

      :crash_and_burn ->
        _a = 1 / 0
        loop()
    end
  end
end

pid = spawn(Process1, :loop, [])
```

```elixir
send(pid, {:spawn_process_2, self()})

process2_pid =
  receive do
    {:process2_pid, p} -> p
  end
```

```elixir
{Process.alive?(pid), Process.alive?(process2_pid)}
```

```elixir
send(process2_pid, :crash_and_burn)
```

```elixir
{Process.alive?(pid), Process.alive?(process2_pid)}
```

### Correction!

The problem with the code above is that we first spawn a process and then we link it.
**That spawned process could crash before a link is established!**.
To solve that problem, the platform offers
`spawn_link` - an atomic opertion which spawns a process and links it in one go.
With `spawn_link` the code above looks as follows:

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit ->
        :ok

      {:spawn_process_2, sender} ->
        process2_pid = spawn_link(Process2, :loop, [])
        send(sender, {:process2_pid, process2_pid})
        loop()
    end
  end
end
```

If you've already done some OTP coding you've seen that a convention for naming functions which start
new genservers or supervisors is `start_link`. Now you know why. `start_link` means
*start a new process linked to the current process*.

## Links are Bidirectional

Instead of crashing the process spawned from module `Process2`, like we did in
the previous section, let's crash the other process, the one spawned from module `Process1`:

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit ->
        :ok

      {:spawn_process_2, sender} ->
        process2_pid = spawn_link(Process2, :loop, [])
        send(sender, {:process2_pid, process2_pid})
        loop()

      :crash_and_burn ->
        _a = 1 / 0
        loop()
    end
  end
end

defmodule Process2 do
  def loop do
    receive do
      :quit -> :ok
    end
  end
end

pid = spawn(Process1, :loop, [])
```

```elixir
send(pid, {:spawn_process_2, self()})

process2_pid =
  receive do
    {:process2_pid, p} -> p
  end
```

```elixir
{Process.alive?(pid), Process.alive?(process2_pid)}
```

```elixir
send(pid, :crash_and_burn)
```

```elixir
{Process.alive?(pid), Process.alive?(process2_pid)}
```

Links are bidirectional!

Imagine that process A is linked to process B, and B is linked to process C, and so on.
If any process in this set crashes, all processes in the link set will eventually crash, too, even if
there is no direct link between some of these processes.

In OTP we have supervisors, one of the functions of supervisors
to form boundaries between crashing processes, and to manage crashes.
How exactly supervisors do it is shown in the next section.

## Trapping Exits Of Linked Processes

Instead of crashing on an exit of a linked process, we can give a process a *superpower* to intercept exits as
messages
with information about a crashed linked process.

The message is a 3-element tuple:
`{:EXIT, pid, reason}` where

* `pid` is the pid of the linked process which exited
* `reason` is a data structure describing why that process crashed

The type spec of this message is:

```
{:EXIT, pid(), any()}
```

```elixir
defmodule Process1 do
  def init do
    # ↓ ↓ ↓ superpower!
    Process.flag(:trap_exit, true)
    loop(nil)
  end

  def loop(state) do
    receive do
      :quit ->
        :ok

      {:spawn_process_2, sender} ->
        process2_pid = spawn_link(Process2, :loop, [])
        send(sender, {:process2_pid, process2_pid})
        loop(state)

      # waiting for an EXIT message!
      {:EXIT, pid, reason} ->
        new_state = %{linked_process_pid: pid, exit_reason: reason}
        loop(new_state)

      # send the state to a sender process
      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

defmodule Process2 do
  def loop do
    receive do
      :quit ->
        :ok

      :crash_and_burn ->
        _a = 1 / 0
        loop()
    end
  end
end

pid = spawn(Process1, :init, [])
```

```elixir
send(pid, {:spawn_process_2, self()})

process2_pid =
  receive do
    {:process2_pid, p} -> p
  end
```

```elixir
send(process2_pid, :crash_and_burn)
```

```elixir
{Process.alive?(pid), Process.alive?(process2_pid)}
```

```elixir
send(pid, {:show_state, self()})

receive do
  {:state, s} -> s
end
```

## Normal Exits

Actually, a process can trap not only crashes of linked processes. A process can trap normal exits,
when a linked process finishes and exits *"peacefully and patriotically"* ©.

In this case `:reason` is always atom `:normal`.

```elixir
defmodule Process1 do
  def init do
    Process.flag(:trap_exit, true)

    spawn_link(fn ->
      :do_nothing_end_exit
    end)

    loop(nil)
  end

  def loop(state) do
    receive do
      :quit ->
        :ok

      {:EXIT, pid, reason} ->
        new_state = %{linked_process_pid: pid, exit_reason: reason}
        loop(new_state)

      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

pid = spawn(Process1, :init, [])
```

```elixir
send(pid, {:show_state, self()})

receive do
  {:state, state} -> state
end
```

## Killing a Process Which Does Not Trap Exit Signals

It is possible to kill a process with `Process.exit(pid, reason)`:

```elixir
defmodule Process1 do
  def loop do
    receive do
      _ -> loop()
    end
  end
end

pid = spawn(Process1, :loop, [])
```

```elixir
Process.alive?(pid)
```

```elixir
Process.exit(pid, :please_die)
```

```elixir
Process.alive?(pid)
```

## Process.exit() For Processes Which Trap Exit Signals

But if we set the process to trap exit signals it will not die.
Instead it will be receiving exit signals

```elixir
defmodule Process1 do
  def init do
    Process.flag(:trap_exit, true)
    loop(nil)
  end

  def loop(state) do
    receive do
      {:EXIT, pid, reason} ->
        new_state = %{linked_process_pid: pid, exit_reason: reason}
        loop(new_state)

      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

pid = spawn(Process1, :init, [])
```

```elixir
Process.exit(pid, :please_die)
```

```elixir
Process.alive?(pid)
```

```elixir
send(pid, {:show_state, self()})

receive do
  {:state, state} -> state
end
```

## Killing a Process Which Traps Exit Signals

Message `:kill` is special and stops even processes which trap exits.

```elixir
defmodule Process1 do
  def init do
    Process.flag(:trap_exit, true)
    loop(nil)
  end

  def loop(state) do
    receive do
      {:EXIT, pid, reason} ->
        new_state = %{linked_process_pid: pid, exit_reason: reason}
        loop(new_state)

      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

pid = spawn(Process1, :init, [])
```

```elixir
Process.alive?(pid)
```

```elixir
Process.exit(pid, :kill)
```

```elixir
Process.alive?(pid)
```

## Summary

* Links are bidirectional
* If a process crashes, all linked processes crash, too
* Unless flag `trap_exit` is set to `true`, in that case a message is sent
* `Process.exit` kills processes where `trap_exit` is NOT set to `true`
* `Process.exit(pid, :kill)` kills processes where `trap_exit` is set to `true`

## Monitors: A Simple Example

* Monitors are not bidirectional: there is a monitored process and a monitoring process
* A monitor between 2 processes does not cause a monitoring process to crash or exit
* Each monitor has a corresponding unique reference which can be stored in ETS or Mnesia or elsewhere

```elixir
defmodule Process1 do
  def loop do
    receive do
      :crash_and_burn ->
        _a = 1 / 0
        loop()
    end
  end
end

defmodule Process2 do
  def init(pid_to_monitor) do
    _reference = Process.monitor(pid_to_monitor)
    loop(nil)
  end

  def loop(state) do
    receive do
      {:DOWN, reference, :process, pid_that_died, reason} ->
        new_state = %{pid_that_died: pid_that_died, exit_reason: reason, reference: reference}
        loop(new_state)

      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

pid1 = spawn(Process1, :loop, [])

pid2 = spawn(Process2, :init, [pid1])
```

```elixir
{Process.alive?(pid1), Process.alive?(pid2)}
```

```elixir
send(pid1, :crash_and_burn)
```

```elixir
{Process.alive?(pid1), Process.alive?(pid2)}
```

```elixir
send(pid2, {:show_state, self()})

receive do
  {:state, state} -> state
end
```

## Monitoring Normal Exits

If a process exits normally, monitoring pocesses receive messages, too.

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit -> :ok
    end
  end
end

defmodule Process2 do
  def init(pid_to_monitor) do
    _reference = Process.monitor(pid_to_monitor)
    loop(nil)
  end

  def loop(state) do
    receive do
      {:DOWN, reference, :process, pid_that_died, reason} ->
        new_state = %{pid_that_died: pid_that_died, exit_reason: reason, reference: reference}
        loop(new_state)

      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

pid1 = spawn(Process1, :loop, [])

pid2 = spawn(Process2, :init, [pid1])
```

```elixir
{Process.alive?(pid1), Process.alive?(pid2)}
```

```elixir
send(pid1, :quit)
```

```elixir
{Process.alive?(pid1), Process.alive?(pid2)}
```

```elixir
send(pid2, {:show_state, self()})

receive do
  {:state, state} -> state
end
```

## Using References To Catch Exits From Specific Monitored Processes

Each monitor has a unique identifier. These identifiers have their own type: `Reference`.
A `Reference` value is guaranteed to be unique among all connected nodes in an Erlang cluster.

You can use a reference to expect a message from a specific monitored process.

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit -> :ok
    end
  end
end

defmodule Process2 do
  def init(pid1_to_monitor, pid2_to_monitor) do
    reference1 = Process.monitor(pid1_to_monitor)
    reference2 = Process.monitor(pid2_to_monitor)
    state = %{reference1: reference1, reference2: reference2, who_died: nil}
    loop(state)
  end

  def loop(state) do
    %{reference1: reference1, reference2: reference2} = state

    receive do
      {:DOWN, ^reference1, :process, _pid_that_died, _reason} ->
        new_state = %{state | who_died: "Process 1 died"}
        loop(new_state)

      {:DOWN, ^reference2, :process, _pid_that_died, _reason} ->
        new_state = %{state | who_died: "Process 2 died"}
        loop(new_state)

      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

pid1 = spawn(Process1, :loop, [])
pid2 = spawn(Process1, :loop, [])

pid = spawn(Process2, :init, [pid1, pid2])
```

```elixir
{Process.alive?(pid1), Process.alive?(pid2), Process.alive?(pid)}
```

```elixir
send(pid2, :quit)
```

```elixir
send(pid, {:show_state, self()})

receive do
  {:state, state} -> state
end
```

## Demonitoring

It is possible to stop monitoring a process.

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit -> :ok
    end
  end
end

defmodule Process2 do
  def init(pid_to_monitor) do
    reference = Process.monitor(pid_to_monitor)
    loop(%{reference: reference, down_message_received: false})
  end

  def loop(state) do
    receive do
      {:DOWN, _reference, :process, _pid_that_died, _reason} ->
        new_state = %{state | down_message_received: true}
        loop(new_state)

      :demonitor ->
        Process.demonitor(state[:reference])
        new_state = %{state | reference: nil}
        loop(new_state)

      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

pid1 = spawn(Process1, :loop, [])

pid2 = spawn(Process2, :init, [pid1])
```

```elixir
{Process.alive?(pid1), Process.alive?(pid2)}
```

```elixir
send(pid2, :demonitor)
```

```elixir
send(pid1, :quit)
```

```elixir
{Process.alive?(pid1), Process.alive?(pid2)}
```

```elixir
send(pid2, {:show_state, self()})

receive do
  {:state, state} -> state
end
```

## Two Monitors Between Two Processes

```elixir
defmodule Process1 do
  def loop do
    receive do
      :quit -> :ok
    end
  end
end

defmodule Process2 do
  def init(pid_to_monitor) do
    reference1 = Process.monitor(pid_to_monitor)
    reference2 = Process.monitor(pid_to_monitor)
    state = %{reference1: reference1, reference2: reference2, messages: []}
    loop(state)
  end

  def loop(state) do
    %{reference1: reference1, reference2: reference2} = state

    receive do
      {:DOWN, ^reference1, :process, _pid_that_died, _reason} ->
        new_state = %{state | messages: ["Message from ref 1" | state[:messages]]}
        loop(new_state)

      {:DOWN, ^reference2, :process, _pid_that_died, _reason} ->
        new_state = %{state | messages: ["Message from ref 2" | state[:messages]]}
        loop(new_state)

      {:show_state, sender} ->
        send(sender, {:state, state})
        loop(state)
    end
  end
end

pid_to_monitor = spawn(Process1, :loop, [])

pid = spawn(Process2, :init, [pid_to_monitor])
```

```elixir
{Process.alive?(pid), Process.alive?(pid_to_monitor)}
```

```elixir
send(pid_to_monitor, :quit)
```

```elixir
{Process.alive?(pid), Process.alive?(pid_to_monitor)}
```

```elixir
send(pid, {:show_state, self()})

receive do
  {:state, state} -> state
end
```

## Links vs Monitors

Links are most often used to create a structure of your application, indirectly via supervisors and the
OTP supervision tree.

Monitors are used a lot when one process need to be aware of what is going on with another process for a
short period of time.

A good example of using a monitor is when we talk to a genserver and use `Genserver.call()`.
If an error happens in that genserver, the error is propagated to the process from which we are calling
`Genserver.call()`. It seems natural after coming from mainstream languages, but why does it happen?
The process of the genserver and the client process might be totally isolated and have no link between them.

The answer lies [here](https://github.com/erlang/otp/blob/master/lib/stdlib/src/gen.erl#L204).

## Summary

* Monitors are unidirectional
* A monitoring process does not crash, unless you code it to crash
* Each monitor has a unique identifier of type `Reference` which you can use to receive messages
  from a specific process, and not from the whole world
* Even though you can unlink a link between two processes, just like you can demonitor a monitor,
  demonitoring of monitors is common pattern in OTP when one process needs to be aware of what's happening
  in another process for a short period of time.

## You Own Supervisor

In the first code chunk we perform some magic with Erlang I/O in order to see what is printed to
STDOUT in Livebook. If you want to understand, load and study
[this livebook](https://raw.githubusercontent.com/jonatanklosko/notebooks/main/notebooks/io_architecture.livemd).

```elixir
defmodule IODev do
  @moduledoc """
  A simple IO device implementing the The Erlang I/O Protocol.
  """

  use GenServer

  def start_link() do
    GenServer.start_link(__MODULE__, [])
  end

  @doc """
  Lists all IO requests sent to this device.
  """
  def get_requests(pid) do
    GenServer.call(pid, :get_requests)
  end

  @impl true
  def init(_opts) do
    {:ok, %{requests: []}}
  end

  @impl true
  def handle_call(:get_requests, _from, state) do
    {:reply, Enum.reverse(state.requests), %{requests: []}}
  end

  # Below we handle the actual IO requests

  @impl true
  def handle_info({:io_request, from, reply_as, request}, state) do
    # Send reply to the process that requested IO
    send(from, {:io_reply, reply_as, :ok})

    # Store all the incoming requests
    state = update_in(state.requests, &[request | &1])

    {:noreply, state}
  end
end

{:ok, io_dev} = IODev.start_link()
original_group_leader = Process.group_leader()
# Set our custom device as the new group leader
Process.group_leader(self(), io_dev)
```

A worker that won't crash:

```elixir
defmodule CountDown do
  def start_link(times) when times > 0 do
    spawn_link(fn -> loop(times) end)
  end

  def loop(0) do
    IO.puts("exiting...")
  end

  def loop(times) do
    receive do
    after
      100 ->
        IO.puts("good countdown: #{times}")
        loop(times - 1)
    end
  end
end
```

```elixir
CountDown.start_link(10)
```

```elixir
IODev.get_requests(io_dev)
```

A worker that will crash:

```elixir
defmodule BadCountDown do
  def start_link(times) when times > 0 do
    # spawn_link will crash the livebook process if we call start_link directly
    # spawn_link(fn -> loop(times) end)
    spawn(fn -> loop(times) end)
  end

  def loop(0), do: :ok
  def loop(6), do: 1 / 0

  def loop(times) do
    receive do
    after
      100 ->
        IO.puts("bad countdown: #{times}")
        loop(times - 1)
    end
  end
end
```

```elixir
BadCountDown.start_link(10)
```

```elixir
IODev.get_requests(io_dev)
```

This is the API we want to use for our supervisor:

```elixir
SuperNaiveSupervisor.start_link([
  {CountDown, :start_link, [10]},
  {BadCountDown, :start_link, [10]}
])
```

SuperNaiveSupervisor will

* spawn child processes
* if a process exits normally, do nothing
* if a process exits abnormally, restart it

The structure of the supervisor:

```elixir
defmodule SuperNaiveSupervisor do
  def start_link(_child_specifications) do
    # spawn a process of the supervisor
  end

  def init(_child_specifications) do
    # set trap_exit to true
    # spawn and link processes according to the specifications
    # create the state structure which look as follows:
    # %{
    #   #PID<0.260.0> => {CountDown, :start_link, [10]},
    #   #PID<0.261.0> => {BadCountDown, :start_link, [10]}
    # }
    # call loop
  end

  defp start_child(_spec = {_mod, _fun, _args}) do
    # spawn and link a processe according to the specification
  end

  def loop(_state) do
    # receive with 2 clauses:
    # * for a normal exit:
    #   * remove the child spec from the state
    #   * call loop()
    # * for an abnormal exit:
    #   * re-start the child
    #   * update the state
    #   * call loop()
  end
end
```

Let's redefine `BadCountDown` with `spawn_link`, and not just `spawn`:

```elixir
defmodule BadCountDown do
  def start_link(times) when times > 0 do
    spawn_link(fn -> loop(times) end)
  end

  def loop(0), do: :ok
  def loop(6), do: 1 / 0

  def loop(times) do
    receive do
    after
      100 ->
        IO.puts("bad countdown: #{times}")
        loop(times - 1)
    end
  end
end
```

Functional supervisor:

```elixir
defmodule SuperNaiveSupervisor do
  def start_link(child_specifications) do
    spawn(__MODULE__, :init, [child_specifications])
  end

  def init(child_specifications) do
    Process.flag(:trap_exit, true)

    state =
      child_specifications
      |> Enum.map(&start_child/1)
      |> Enum.into(Map.new())

    loop(state)
  end

  defp loop(state) do
    receive do
      {:EXIT, pid_just_exited, :normal} ->
        IO.puts(["child ", inspect(pid_just_exited), " has exited normally"])
        new_state = Map.delete(state, pid_just_exited)
        loop(new_state)

      {:EXIT, pid_just_exited, _reason} ->
        {:ok, child_spec} = Map.fetch(state, pid_just_exited)

        IO.puts(["restarting", inspect(child_spec, charlists: :as_lists)])
        {new_pid, _} = start_child(child_spec)

        new_state =
          state
          |> Map.delete(pid_just_exited)
          |> Map.put(new_pid, child_spec)

        loop(new_state)
    end
  end

  defp start_child(spec = {mod, fun, args}) do
    # where is linking and spawning?
    case apply(mod, fun, args) do
      pid when is_pid(pid) -> {pid, spec}
      _ -> :error
    end
  end
end
```

Let's test it with a child that exits normally:

```elixir
SuperNaiveSupervisor.start_link([
  {CountDown, :start_link, [10]}
])
```

```elixir
IODev.get_requests(io_dev)
```

And now with a bad process:

```elixir
sup_pid =
  SuperNaiveSupervisor.start_link([
    {CountDown, :start_link, [10]},
    {BadCountDown, :start_link, [10]}
  ])
```

```elixir
IODev.get_requests(io_dev)
```

The supervisor keeps restarting the bad child, so let's clean up by killing the supervisor:

```elixir
Process.exit(sup_pid, :kill)
```

## Supervisor Exits After Too Many Restarts Of A Child

The code above has many problems.

One of them is that the supervisor tries restarting a failing child forever.
This is not what supervisors do.

Let's make sure that the supervisor tries restarting a failing child N times and
if the problem persists, the supervisor exits.

```elixir
defmodule SuperNaiveSupervisor do
  @max_restarts 3

  def start_link(child_specifications) do
    spawn(__MODULE__, :init, [child_specifications])
  end

  def init(child_specifications) do
    Process.flag(:trap_exit, true)

    state =
      child_specifications
      |> Enum.map(&start_child/1)
      |> Enum.map(fn {pid, spec} -> {pid, {spec, @max_restarts}} end)
      |> Enum.into(Map.new())

    # %{
    #   #PID<0.260.0> => {{CountDown, :start_link, [10]}, 3},
    #   #PID<0.261.0> => {{BadCountDown, :start_link, [10]}, 3}
    # }

    loop(state)
  end

  defp loop(state) do
    receive do
      {:EXIT, pid_just_exited, :normal} ->
        IO.puts(["child ", inspect(pid_just_exited), " has exited normally"])
        new_state = Map.delete(state, pid_just_exited)
        loop(new_state)

      {:EXIT, pid_just_exited, reason} ->
        case Map.fetch(state, pid_just_exited) do
          {:ok, {_child_spec, 0}} ->
            reason = {:sup_giving_up, reason}
            IO.puts("Supervisor giving up, reason:  #{inspect(reason)}}")
            exit(reason)

          {:ok, {child_spec, counter}} ->
            IO.puts(["restarting", inspect(child_spec, charlists: :as_lists)])
            {new_pid, _} = start_child(child_spec)

            new_state =
              state
              |> Map.delete(pid_just_exited)
              |> Map.put(new_pid, {child_spec, counter - 1})

            loop(new_state)
        end
    end
  end

  defp start_child(spec = {mod, fun, args}) do
    case apply(mod, fun, args) do
      pid when is_pid(pid) -> {pid, spec}
      _ -> :error
    end
  end
end
```

```elixir
sup_pid =
  SuperNaiveSupervisor.start_link([
    {BadCountDown, :start_link, [10]}
  ])
```

```elixir
IODev.get_requests(io_dev)
```

## Treating Process.exit() Differently

Another problem is  that if we kill a child with `Process.exit()`,
the supervisor will restart it!

We need a way to differentiate between abnormal exits and exits
caused by `Process.exit()`.

This is possible because the reason in exits caused by `Process.exit()` is `killed`.

```elixir
defmodule SuperNaiveSupervisor do
  @max_restarts 3

  def start_link(child_specifications) do
    spawn(__MODULE__, :init, [child_specifications])
  end

  def init(child_specifications) do
    Process.flag(:trap_exit, true)

    state =
      child_specifications
      |> Enum.map(&start_child/1)
      |> Enum.map(fn {pid, spec} -> {pid, {spec, @max_restarts}} end)
      |> Enum.into(Map.new())

    loop(state)
  end

  defp loop(state) do
    receive do
      {:EXIT, pid_just_exited, :normal} ->
        IO.puts(["child ", inspect(pid_just_exited), " has exited normally"])
        new_state = Map.delete(state, pid_just_exited)
        loop(new_state)

      # # !!!
      {:EXIT, pid_just_exited, :killed} ->
        IO.puts(["child ", inspect(pid_just_exited), " has been killed"])
        new_state = Map.delete(state, pid_just_exited)
        loop(new_state)

      {:EXIT, pid_just_exited, reason} ->
        case Map.fetch(state, pid_just_exited) do
          {:ok, {_child_spec, 0}} ->
            reason = {:sup_giving_up, reason}
            IO.puts("Supervisor giving up, reason:  #{inspect(reason)}}")
            exit(reason)

          {:ok, {child_spec, counter}} ->
            IO.puts(["restarting", inspect(child_spec, charlists: :as_lists)])
            {new_pid, _} = start_child(child_spec)

            new_state =
              state
              |> Map.delete(pid_just_exited)
              |> Map.put(new_pid, {child_spec, counter - 1})

            loop(new_state)
        end
    end
  end

  defp start_child(spec = {mod, fun, args}) do
    case apply(mod, fun, args) do
      pid when is_pid(pid) -> {pid, spec}
      _ -> :error
    end
  end
end
```

## Restart Strategies

We have just implemented restart strategy `:one_for_one`

* `:one_for_one` - if a child process terminates,
  only that process is restarted.
* `:one_for_all` - if a child process terminates,
  all other child processes are terminated and then
  all child processes (including the terminated one) are restarted.
* `:rest_for_one` - if a child process terminates,
  the terminated child process and the rest of the children started after it, are terminated and restarted.

Homework:

1. Read (or re-read)
   [the supervisors documentation](https://hexdocs.pm/elixir/1.12/Supervisor.html#module-strategies),
   especially the part about
   [restart strategies](https://hexdocs.pm/elixir/1.12/Supervisor.html#module-strategies)
   and options `max_restarts` and `max_seconds`
2. Implement restart strategy `:one_for_all`
